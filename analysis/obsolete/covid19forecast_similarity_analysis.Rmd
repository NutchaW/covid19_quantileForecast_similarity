---
title: "COVID-19 Forecast Similarity Analysis"
author: "Johannes Bracher, Evan Ray, Nick Reich, Nutcha Wattanachit"
date: "09/23/2021"
header-includes:
   - \usepackage{tabularx}
   - \usepackage{hyperref}
   - \usepackage{wrapfig}
   - \usepackage{float}
   - \usepackage{colortbl}
   - \usepackage{pdflscape}
   - \usepackage{tabu}
   - \usepackage{xcolor}
output:
  pdf_document:
        latex_engine: xelatex
---

```{r setup, include=FALSE}
library(tidyverse)
library(energy)
library(knitr)
library(data.table)
library(covidHubUtils)
library(RColorBrewer)
#devtools::install_github("reichlab/covidHubUtils",force=TRUE)
library(lubridate)
library(zoltr)
library(igraph)
library(gtools)
library(gtable)
library(gridExtra)
library(grid)
library(ggdendro)
library(gridGraphics)
knitr::opts_chunk$set(echo=FALSE,
                       comment = FALSE, message=FALSE, fig.show= 'hold',fig.pos="H",table.placement="H",
                       fig.align = 'center',warning =FALSE)
```

# Forecast inclusion criteria

* Models: All models with complete submissions for the following criteria
* Targets: 1-4 wk ahead inc death and inc case
* Target end dates: Oct 19th, 2020 - Sep 11th,2021
* Probability levels: All
* Locations:
   + 5 states with highest cumulative deaths by February 27th, 2021: CA, FL, NY, PA, TX
   + 5 states with highest cumulative cases by February 27th, 2021: CA, FL, IL, NY, TX
   + 5 states with lowest cumulative deaths by February 27th, 2021: AK, HI, ME, VT, WY
   + 5 states with lowest cumulative cases by February 27th, 2021: DC, HI, ME, VT, WY
   
# 1-4 Week Ahead Incident Death Forecasts

<!--cramer distance offer more direct details about dissimilarity between two forecasts than WIS, whose decomposition is wrt to truth--->



```{r,cache=TRUE}
source("../functions/distance_func_script.R")
# set targets for analysis
target_horizon <- 1:4
target_var <- c("inc death","inc case")
# # read in model metadata
# metadata <- read.csv("../metadata_categorized.csv") 
# # one off change
# metadata$compartmental[which(metadata$team_name == "Karlen Working Group")] <- TRUE
# metadata$compartmental[which(metadata$team_name == "UCSD_NEU")] <- TRUE
# metadata$compartmental[which(metadata$team_name == "Robert Walraven")] <- FALSE
# metadata$JHU_data[which(metadata$team_name == "COVID-19 Forecast Hub")] <- TRUE
# metadata$ensemble <- ifelse(metadata$ensemble==TRUE,1,0)
# metadata$compartmental <- ifelse(metadata$compartmental==TRUE,1,0)
# metadata$stats <- ifelse(metadata$stats==TRUE,1,0)
# # manual change 
# # add text columns
# metadata$model_type <- ifelse(metadata$ensemble, 
#                                   "ensemble", 
#                                   ifelse(metadata$stats + metadata$compartmental==2,
#                                          "both stats and mech",
#                                          ifelse((metadata$stats*2)+metadata$compartmental==2,
#                                                 "statistical",
#                                                 ifelse((metadata$stats*2)+metadata$compartmental==1,
#                                                        "mechanistic",
#                                                        "neither stats nor mech"))))
# metadata$data_source <- ifelse(metadata$mobility_data,"use mobility data","do not use mobility data")

# each target will have different sets of models based on the current filtering
## high count
wide_frame_death <- read.csv("../data/quantile_frame.csv") %>%
  dplyr::filter(!(model %in% c("CU-nochange","CU-scenario_high","CU-scenario_low","CU-scenario_mid")))
wide_frame_case <- read.csv("../data/quantile_frame_inc.csv") %>%
  dplyr::filter(!(model %in% c("CU-nochange","CU-scenario_high","CU-scenario_low","CU-scenario_mid")))
d_frame <- frame_format(wide_frame_death) 
c_frame <- frame_format(wide_frame_case) 
# # model type
# d_meta <- colnames(d_frame)[-c(1:6)]
# c_meta <- colnames(c_frame)[-c(1:6)]
# d_meta_low <- colnames(d_frame_low)[-c(1:6)]
# 
# 
# high_truth <- read.csv("../data/truth_deathcase_h.csv") %>%
#   dplyr::mutate(target_end_date_x = as.character(as.Date(target_end_date)+7)) %>%
#   dplyr::select(-c("X","target_end_date")) 
# high_truth <- high_truth[,c(1:2,ncol(high_truth),4:ncol(high_truth)-1)] 
```

```{r,cache=TRUE}
# high count death forecast
#loc_name <-  unique(wide_frame_death[,c("abbreviation","location")])
d_metadata <- metadata %>%
  dplyr::filter(model_abbr %in% d_meta)
recent_dmeta <- d_metadata %>%
  dplyr::group_by(team_name,model_name) %>%
  dplyr::filter(date==max(as.POSIXct(date))) %>%
  dplyr::ungroup() %>%
  dplyr::group_by(model_abbr) %>%
  dplyr::filter(date==max(as.POSIXct(date))) %>%
  dplyr::ungroup()
short_meta <- recent_dmeta[,c(3,ncol(recent_dmeta)-1)]
short_data <- recent_dmeta[,c(3,ncol(recent_dmeta))]
# calculate distance matrices
q_set <- unique(d_frame$quantile) 
approx_cd_list <- build_distance_frame(d_frame, 
                           horizon_list=c(1:4),
                           target_list="inc death",
                           approx_rule="trapezoid_riemann",
                           tau_F=q_set,tau_G=q_set)

# approx_cd_list2 <- suppressWarnings(build_distance_frame(d_frame, 
#                                         horizon_list=c(1:4),
#                                         target_list="inc death",
#                                         approx_rule="approximation2",
#                                         tau_F=q_set,tau_G=q_set))
# extract data
total_frame <- approx_cd_list[[1]] %>%
  dplyr::mutate(pair=paste(model_1,model_2,sep=" vs ")) %>%
  dplyr::group_by(horizon,target_variable,target_end_date,pair) %>%
  dplyr::mutate(mean_approx_cd=mean(approx_cd)) %>%
  dplyr::ungroup() %>%
  dplyr::select(-c("approx_cd","location")) %>%
  dplyr::distinct()
total_frame$target_end_date <- as.Date(total_frame$target_end_date,origin="1970-01-01")

d_frame_mean <- lapply(1:4, function(x) cd_matrix(approx_cd_list[[3]],x))
# low count
d_metadata_low <- metadata %>%
  dplyr::filter(model_abbr %in% d_meta_low)
recent_dmeta_low <- d_metadata_low %>%
  dplyr::group_by(team_name,model_name) %>%
  dplyr::filter(date==max(as.POSIXct(date))) %>%
  dplyr::ungroup() %>%
  dplyr::group_by(model_abbr) %>%
  dplyr::filter(date==max(as.POSIXct(date))) %>%
  dplyr::ungroup()
short_meta_low <- recent_dmeta_low[,c(3,ncol(recent_dmeta_low)-1)]
approx_cd_list_low <- build_distance_frame(d_frame_low, 
                           horizon_list=c(1:4),
                           target_list="inc death",
                           approx_rule="trapezoid_riemann",
                           tau_F=q_set,tau_G=q_set)
# extract data
total_frame_low <- approx_cd_list_low[[1]] %>%
  dplyr::mutate(pair=paste(model_1,model_2,sep=" vs ")) %>%
  dplyr::group_by(horizon,target_variable,target_end_date,pair) %>%
  dplyr::mutate(mean_approx_cd=mean(approx_cd)) %>%
  dplyr::ungroup() %>%
  dplyr::select(-c("approx_cd","location")) %>%
  dplyr::distinct()
total_frame_low$target_end_date <- as.Date(total_frame_low$target_end_date,origin="1970-01-01")

d_frame_mean_low <- lapply(1:4, function(x) cd_matrix(approx_cd_list_low[[3]],x))

#----------------------- scaled  forecasts-------------------------------------------------##
# make scale data
scaled_d_frame <- d_frame %>%
  dplyr::mutate(target_end_date_x=as.character(as.Date(target_end_date)-(7*(horizon-1)))) %>%
  dplyr::left_join(high_truth[,2:5]) %>%
  dplyr::select(-"target_end_date_x")
scaled_d_frame <- na.omit(scaled_d_frame) 
scaled_d_frame[,7:ncol(scaled_d_frame)-1] <- sapply(7:ncol(scaled_d_frame)-1, 
                                                    function(x) scaled_d_frame[,x]/scaled_d_frame[,ncol(scaled_d_frame)])
scaled_d_frame <- scaled_d_frame[,1:ncol(scaled_d_frame)-1]
#  scaled distances
approx_cd_list_scaled <- build_distance_frame(scaled_d_frame, 
                           horizon_list=c(1:4),
                           target_list="inc death",
                           approx_rule="trapezoid_riemann",
                           tau_F=q_set,tau_G=q_set)
total_frame_scaled <- approx_cd_list_scaled[[1]] %>%
  dplyr::mutate(pair=paste(model_1,model_2,sep=" vs ")) %>%
  dplyr::group_by(horizon,target_variable,target_end_date,pair) %>%
  dplyr::mutate(mean_approx_cd=mean(approx_cd)) %>%
  dplyr::ungroup() %>%
  dplyr::select(-c("approx_cd","location")) %>%
  dplyr::distinct()
total_frame_scaled$target_end_date <- as.Date(total_frame_scaled$target_end_date,origin="1970-01-01")

d_frame_mean_scaled <- lapply(1:4, function(x) cd_matrix(approx_cd_list_scaled[[3]],x))
```

There are 13 models that fulfilled the criteria for the 5 locations with highest cumulative deaths. Below is an example of approximated CramÃ©r distances between COVIDhub-ensemble and UMass-Mechbayes for 1-4 week ahead forecasts of incident deaths in CA in the week of 01/11/2021. It is evident from the approx. CD and the $95\%$ prediction interval that Mechbayes forecasts for this peak week are more dispersive compared to the ensemble forecasts (not surprising considering "stacking" lacks dispersion?). 

```{r,fig.align='center'}
# proof of concept 
fdat <- load_latest_forecasts(models = c("COVIDhub-ensemble","UMass-MechBayes"),
                              last_forecast_date = "2021-01-11",
                              source = "zoltar",
                              forecast_date_window_size = 6,
                              locations = "06",
                              types = c("quantile", "point"), 
                              verbose = FALSE,
                              targets = paste(1:4, "wk ahead inc death"))

p <- plot_forecasts(fdat, 
               target_variable = "inc death", 
               truth_source = "JHU",
               intervals = c(.95), 
               facet = model~.,
               fill_by_model = TRUE,
               facet_ncol=1,
               plot=FALSE,
               title="none",
               subtitle="none",
               show_caption=FALSE) 

p_formatted <- p + 
  scale_x_date(name=NULL, date_breaks = "1 months", date_labels = "%b-%y",
               limits = c(as.Date("2020-10-17"), as.Date("2021-05-24")))+
  theme(axis.text.x = element_text(angle=90, hjust=-0.2),
        legend.position = "none")

for(i in 1:4){
    p <- d_frame %>%
      dplyr::filter(horizon==i,
                    target_end_date==as.Date("2021-01-16")+(7*(i-1)),
                    location==6) %>%
      .[,grep("COVIDhub-ensemble|UMass-MechBayes|quantile",colnames(d_frame))] 
    assign(paste0("p",i),
           plot_step(p,"COVIDhub-ensemble","UMass-MechBayes",i)
           )
}
legend = gtable_filter(ggplot_gtable(ggplot_build(p1)), "guide-box")
# grid.arrange(p_formatted,p1,p2,p3,p4, layout_matrix = lay)
grid.arrange(p_formatted, 
             arrangeGrob(p1 + theme(legend.position="none"),
                         p2 + theme(legend.position="none"), 
                         p3 + theme(legend.position="none"),
                         p4 + theme(legend.position="none"),
                         legend,
                         heights=c(1,1,1,1, 0.3),
                         ncol = 1),
             ncol=2,top = "Location: CA, Forecast date: 01/11/2021")
```

## Mean approximated pairwise distances across 5 high count locations

We can visualize the mean approximated pairwise distances across all weeks and locations in heatmaps. The distance from the model to itself is zero. The $x-$axis is arranged based in an ascending order of the model's approximate pairwise distance from the COVIDhub-ensemble. So, the first model is the model that is most dissimilar (on average) to the ensemble in this time frame.

For high morality count locations, the distances between pairs of forecasts are higher for further forecast horizons. 
CU-select and RobertWalraven-ESG forecasts for high count locations seem to be more dissimilar to other models on average across all forecast horizons. After scaling by the truth at $t-1$ from the forecast week, some distances get "smoothed out", but the observed trends persist. 


```{r,fig.align='center'}
distance_heatmap(approx_cd_list[[3]],
                 "Mean Approx. CD of Inc Death Forecasts by Horizon",
                 recent_dmeta)
distance_heatmap(approx_cd_list_scaled[[3]],
                 "Mean Approx. CD of Scaled Inc Death Forecasts by Horizon",
                 recent_dmeta)
```

## Approximated pairwise distances over time, across 5 high count locations

### Dissimilarity to COVIDhub-ensemble

We can also look at the mean approximated pairwise distances across locations only to see how the models become more similar or dissimilar over time. Across all horizons, we see a large range of distances from the ensemble forecasts around the second week of January 2021. After scaling, the dissimilarity is attenuated. Specifically, the peak of dissimilarity is shifted to early/mid December across all horizons. The large range of dissimilarity is also observed for about a month after for the 4 week ahead horizon. This might be due to the higher uncertainty exhibited in longer horizon forecasts as we were about to observe the peak month?

```{r,fig.align='center',fig.align='center'}
ot_data <- total_frame %>% 
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="COVIDhub-ensemble",
                model_2 != "COVIDhub-ensemble") %>%
  dplyr:: ungroup() 
ot_data_scaled <- total_frame_scaled %>% 
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="COVIDhub-ensemble",
                model_2 != "COVIDhub-ensemble") %>%
  dplyr:: ungroup() 

box1 <- catbox_plot(ot_data,
        "Mean Approx. CD") 
scaled_box1 <- catbox_plot(ot_data_scaled,
        "Scaled Mean Approx. CD")
grid.arrange(box1,scaled_box1,
             top="Mean Approx. CD from COVIDhub-ensemble - High Mortality Count Locations",
             ncol=2)
```
\newpage

### The week of high incident deaths and forecast similarities to the COVIDhub-ensemble forecasts for CA

This boxplot shows that we see more dissimilarity from the ensemble forecasts during the week that the actual death number is high compared to the week that we observed lower deaths. I assume this is probably the case for all high count locations, but I have yet to generate the plots for the combined locations. However, this is attenuated after scaling. The median CDs are more similar after scaling, but the range of scaled CDs in the week with higher deaths are still larger. We might want to check the standard deviation here?

```{r,fig.align='center'}
# proof of concept 
fdat2 <- load_forecasts(models = "COVIDhub-ensemble",
                        forecast_dates = c("2021-01-11","2021-04-26"),
                        source = "zoltar",
                        locations = "06",
                        types = c("quantile", "point"), 
                        verbose = FALSE,
                        targets = paste(1:4, "wk ahead inc death"))

p2 <- plot_forecasts(fdat2, 
               target_variable = "inc death", 
               truth_source = "JHU",
               intervals = c(.95), 
               plot=FALSE,
               title="none",
               subtitle="none",
               show_caption=FALSE) 
p_formatted2 <- p2 + 
  scale_x_date(name=NULL, date_breaks = "1 months", date_labels = "%b-%y",
               limits = c(as.Date("2020-10-17"), as.Date("2021-05-24")))+
  theme(axis.text.x = element_text(angle=90, hjust=-0.2),
        legend.position = "none")

# build boxplots
ot_data2 <- total_frame %>% 
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="COVIDhub-ensemble",
                model_2 != "COVIDhub-ensemble",
                (target_end_date==(as.Date("2021-01-11")-2)+7*horizon|
                   target_end_date==(as.Date("2021-04-26")-2)+7*horizon)) %>%
  dplyr:: ungroup() %>%
  dplyr::mutate(target_end_date=ifelse(target_end_date==(as.Date("2021-01-11")-2)+7*horizon,
                                       "2021-01-11","2021-04-26"))
ot_data_scaled2 <- total_frame_scaled %>% 
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="COVIDhub-ensemble",
                model_2 != "COVIDhub-ensemble",
                (target_end_date==(as.Date("2021-01-11")-2)+7*horizon|
                   target_end_date==(as.Date("2021-04-26")-2)+7*horizon)) %>%
  dplyr:: ungroup() %>%
  dplyr::mutate(target_end_date=ifelse(target_end_date==(as.Date("2021-01-11")-2)+7*horizon,
                                       "2021-01-11","2021-04-26"))

box.e <- catbox_plot2(ot_data2,
        "Mean Approx. CD") 
scaled_box.e <- catbox_plot2(ot_data_scaled2,
        "Scaled Mean Approx. CD")
# combined panels
legend = gtable_filter(ggplot_gtable(ggplot_build(box.e)), "guide-box")
grid.arrange(p_formatted2, 
             arrangeGrob(box.e+ theme(legend.position="none"),
                         scaled_box.e+ theme(legend.position="none"), 
                         legend,
                         heights=c(1,1,0.2),
                         ncol = 1),
             ncol=2,top = "Mean Approx. CD from COVIDhub-ensemble - CA")
```

\newpage

### Disagreement among models and ensemble scores

The y-axis is the approximated Cramer distances from the ensemble for each target end dates in the analysis. Maybe it's better to plot Cramer distances every pairs and not just the model to the ensemble? I see something like "a higher disagreement is associated with a higher ensemble score"?. If forecasts are dissimilar, the ensemble forecasts tend to be less accurate? But both x- and y-axis are affected by the scale of observed values...

```{r,fig.align='center',fig.width=6,fig.height=4}
truth_local <- read.csv("../data/truth_deathcase_h.csv") %>%
  dplyr::select(-"X")

scores <- wide_frame_death %>%
  dplyr::filter(model=="COVIDhub-ensemble") %>%
  score_forecasts(forecasts = .,
                  return_format = "wide",
                  truth = truth_local,
                  use_median_as_point=TRUE) %>%
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::summarise(wis = mean(wis)) 

score_dis <- ot_data %>%
  dplyr::mutate(target_end_date=as.character(target_end_date)) %>%
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::mutate(mean=mean(mean_approx_cd),
                sd=sd(mean_approx_cd)) %>%
  dplyr::left_join(scores,by=c("horizon","target_end_date")) %>%
  select(-c("model_1","model_2","pair","mean_approx_cd")) %>%
  distinct()
scatter(score_dis,"Ensemble score vs Forecast similarity","wis","Ensemble WIS","Approx. CD")
# scores %>%
#   dplyr::filter(model %in% short_meta$model_abbr) %>%
#   dplyr::group_by(model, horizon,target_end_date) %>%
#   dplyr::summarise(wis = mean(wis)) %>%
#   scoringutils::score_heatmap(metric = "wis", x = "horizon")
```


<!-- ### Dissimilarity among all pairs of models -->

<!-- This shows a similar trend to the dissimilarity to the ensemble over time. -->

```{r}
all_data <- total_frame %>%
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 !=model_2) %>%
  dplyr:: ungroup() %>%
  dplyr::distinct(horizon,target_end_date,mean_approx_cd,.keep_all = TRUE )
# 
# all_data_scaled <- total_frame_scaled %>%
#   dplyr::group_by(horizon,target_end_date) %>%
#   dplyr::filter(model_1 !=model_2) %>%
#   dplyr:: ungroup() %>%
#   dplyr::distinct(horizon,target_end_date,mean_approx_cd,.keep_all = TRUE)
# 
# box2 <- catbox_plot(all_data,
#         "Mean Approx. CD")
# scaled_box2 <- catbox_plot(all_data_scaled,
#         "Scaled Mean Approx. CD")
# grid.arrange(box1,scaled_box1,
#              top="Mean Approx. CD - High Mortality Count Locations",
#              ncol=2)
```

\newpage 

### Relationship between observed values and similarities 

Observed incident deaths at $t-1$ seem to be associated with more dissimilarity among forecasts made at time $t$. But we know the scale of observed values impacts the distances. 

```{r,fig.align='center',fig.width=5,fig.height=3}
sum_truth <- truth_local %>%
  dplyr::group_by(target_end_date) %>%
  dplyr::summarise(sum_d=sum(value)) %>%
  ungroup()
 ot <- all_data %>%
  dplyr::mutate(forecast_week=as.character((as.Date(target_end_date)-(7*horizon)))) %>%
  dplyr::left_join(sum_truth,by=c("forecast_week"="target_end_date")) %>%
  group_by(horizon,forecast_week) %>%
  dplyr::summarise(sim=mean(mean_approx_cd), truth=sum_d) %>%
  ungroup() %>%
  distinct()
scatter(ot,"Observed incident deaths vs Forecast similarity","current","Observed incident deaths","Approx. CD")
# scores %>%
#   dplyr::filter(model %in% short_meta$model_abbr) %>%
#   dplyr::group_by(model, horizon,target_end_date) %>%
#   dplyr::summarise(wis = mean(wis)) %>%
#   scoringutils::score_heatmap(metric = "wis", x = "horizon")
```


Future incident deaths at target end dates seem to be associated with more dissimilarity among forecasts made at time $t$.

```{r,fig.align='center',fig.width=5,fig.height=3}
ot2 <- all_data %>% 
  dplyr::mutate(target_end_date=as.character(target_end_date))%>%
  dplyr::left_join(sum_truth,by="target_end_date") %>%
  group_by(horizon,target_end_date) %>%
  dplyr::summarise(sim=mean(mean_approx_cd), truth=sum_d) %>%
  ungroup() %>%
  distinct()
scatter(ot2,"Future incident deaths vs Forecast similarity","future","Approx. CD","Incident deaths at target end date")
# scores %>%
#   dplyr::filter(model %in% short_meta$model_abbr) %>%
#   dplyr::group_by(model, horizon,target_end_date) %>%
#   dplyr::summarise(wis = mean(wis)) %>%
#   scoringutils::score_heatmap(metric = "wis", x = "horizon")
```

\newpage

## Model types

```{r}
tab1 <- rbind(short_meta) %>%
  distinct()
knitr::kable(tab1, col.names = c("Model","Type"))
```

### Proportions of each model's nearest neighbor being the same type

We have 12 models, not including the ensemble. Each model's nearest neighbor is the model that is most similar to it (not including the model itself and duplicated pairs). Based on the proportions doesn't look like model types impact the similarity of forecasts overall. 

```{r}
prop_dat <- approx_cd_list[[3]] %>%
  dplyr::filter(model_1!="COVIDhub-ensemble",
                model_2!="COVIDhub-ensemble",
                model_1 !=model_2) %>%
  dplyr::group_by(horizon,model_1) %>%
  dplyr::filter(mean_dis==min(mean_dis)) %>%
  dplyr::ungroup() %>%
  dplyr::distinct(horizon,mean_dis,.keep_all = TRUE ) %>%
  dplyr::arrange(horizon,model_1,mean_dis) %>%
  dplyr::left_join(short_meta,by=c("model_1"="model_abbr")) %>%
  dplyr::left_join(short_meta,by=c("model_2"="model_abbr")) %>%
  dplyr::group_by(horizon) %>%
  dplyr::mutate(type=ifelse(model_type.x==model_type.y,1,0),
                nr=1,
                prop=sum(type)/sum(nr)) %>%
  dplyr::ungroup() %>%
  select("horizon","target_variable","prop") %>%
  distinct()
  
knitr::kable(prop_dat, col.names = c("Horizon","Target","Proportion of same-type nearest neighbors"),align="c")
```

<!--not doing the test for now because there seems to be no relationship here-->

<!-- ## Hierarchical clustering based on mean approx. CD across all weeks and locations -->

<!-- We can cluster the distances using hierarchical clustering. Different linkages will result in different clusters - here we use ward linkage.  -->

<!-- ```{r,fig.align='center',fig.cap="High Mortality Count Locations", fig.height=4} -->
<!-- for(i in 1:4){ -->
<!--     assign(paste0("dp_",i), -->
<!--             dendro_plot(i, "d_frame_mean",short_meta) -->
<!--               ) -->
<!--     assign(paste0("dpl_",i), -->
<!--             dendro_plot(i, "d_frame_mean_low",short_meta_low) -->
<!--               ) -->
<!--   }  -->
<!-- grid.arrange(dp_1,dp_2,dp_3,dp_4,nrow=2) -->
<!-- grid.arrange(dpl_1,dpl_2,dpl_3,dpl_4,nrow=2) -->
<!-- ``` -->



