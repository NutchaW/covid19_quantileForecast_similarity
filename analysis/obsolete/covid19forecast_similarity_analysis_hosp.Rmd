---
title: "COVID-19 Forecast Similarity Analysis for Hospitalizations"
author: "Li Shandross, Nutcha Wattanachit, Nick Reich, Evan Ray"
date: "07/30/2021"
header-includes:
   - \usepackage{tabularx}
   - \usepackage{hyperref}
   - \usepackage{wrapfig}
   - \usepackage{float}
   - \usepackage{colortbl}
   - \usepackage{pdflscape}
   - \usepackage{tabu}
   - \usepackage{xcolor}
output:
  pdf_document:
        latex_engine: xelatex
---

```{r setup, include=FALSE}
library(tidyverse)
library(energy)
library(knitr)
library(data.table)
library(covidHubUtils)
#devtools::install_github("reichlab/covidHubUtils")
library(lubridate)
library(zoltr)
library(igraph)
library(gtools)
library(gridExtra)
library(ggdendro)
library(anytime) # note that the Rcpp must be loaded to use the anytime package
library(Rcpp)
library(patchwork)
knitr::opts_chunk$set(echo=FALSE,
                       comment = FALSE, message=FALSE, fig.show= 'hold',fig.pos="H",table.placement='H',
                       fig.align = 'center')
```

```{r functions and targets}
source("../functions/distance_func_script.R")
source("../functions/distance_func_hosp_script.R")

# set targets for analysis
target_horizon <- 1:4
target_var <- "inc hosp"
```


Over the course of the pandemic, many teams across the United States have worked to create forecasting models for Covid-19 cases, deaths, and hospitalizations. We choose to examine models specifically forecasting incident hospitalizations. But how similar are these models? Are there patterns of similarity, perhaps due to data source, modeling technique, or another incorporated factor? Or do models that tend to perform well tend to be similar? These questions are all reasons why we might be interested in measuring the similarities between covid-19 forecasting models. 


## Introduction to Cramér's Distance

Extending the work of Bracher et. al, we select Cramér's Distance as a metric to evaluate the similarity between models. The Cramér's Distance of two predictive distributions $F$ and $G$ is defined as follows: 

\begin{align}
\text{CD}(F, G) &= \int_{-\infty}^\infty(F(x) - G(x))^2 dx
\end{align}

where $F(x)$ and $G(x)$ are the two cumulative distribution functions respectively. 

However, we actually only know $K$ quantiles from $F$ and $G$ rather than their entire distributions. Thus, we must approximate their Cramér's Distance, and we do so by using Bracher et. al's trapezoidal rule: 

\begin{align}
\text{CD}(F,G) &\approx\sum^{2K-1}_{j=1}\int^{q_{j+1}}_{q_j}{F(x)−G(x)}^2\\
&\approx\sum^{2K-1}_{j=1}\frac{\{\hat{F}(q_j)-\hat{G}(q_j)\}^2+\{\hat{F}(q_{j+1})-\hat{G}(q_{j+1})\}^2}{2}(q_{j+1}-q_{j})
\end{align}

where $q_j$ is the jth quantile.

Cramér's Distance (as well as it's approximation) is an ideal quantitative measurement for measuring the similarity of covid-19 forecasting models because it is not only relatively easy to compute but also is compatible with the weighted interval score (WIS) used to score forecasts by the COVID-19 Forecast Hub. 

Below, Figure 1 illustrates how Cramér's Distance is calculated between two models in this analysis. Forecasts made on a single date are split into 1 to 4 week horizons, with Cramér's Distance calculated separately at each horizon.

```{r, out.width="95%", fig.align='center', fig.cap="Cramér's Distance Visualized", message=FALSE, warning=FALSE, comment=FALSE}
fdat <- purrr::map_dfr(c("2021-01-28"), 
                       function(dats){
                         load_latest_forecasts(models = c("COVIDhub-ensemble","Karlen-pypm", "CU-select"),
                                               last_forecast_date = dats,
                                               source = "zoltar",
                                               forecast_date_window_size = 6,
                                               locations = "48",
                                               types = c("quantile", "point"), 
                                               verbose = FALSE,
                                               targets = paste0(1:28," day ahead inc hosp"))}
)

fdat <- fdat %>%
  dplyr::mutate(horizon_week = case_when(
    horizon %in% 1:7 ~ 1,
    horizon %in% 8:14 ~ 2, 
    horizon %in% 15:21 ~ 3,
    horizon %in% 21:28 ~ 4,
  )) 

p_base <- plot_forecasts(fdat, 
                         models = c("COVIDhub-ensemble", "CU-select"),
                         target_variable = "inc hosp", 
                         truth_source = "HealthData",
                         intervals = c(.5, .95), 
                         fill_by_model = TRUE, 
                         plot=FALSE,
                         fill_transparency = 0.6,
                         title = "Daily COVID-19 Inc Hosp Forecasts in TX by Horizon",
                         subtitle = "COVIDhub-ensemble and CU-select",
                         show_caption = FALSE)

p <- p_base + 
  scale_x_date(name=NULL, date_breaks = "1 weeks", date_labels = "%m-%d", 
               limits = c(as.Date("2021-01-01"), as.Date("2021-02-22"))) +
  theme(axis.ticks.length.x = unit(0.5, "cm"),
        axis.text.x = element_text(vjust = 7, hjust = -0.2),
        legend.position = "none")

p_horizon <- plot_forecasts(fdat, 
                         models = c("COVIDhub-ensemble", "CU-select"),
                         target_variable = "inc hosp", 
                         truth_source = "HealthData",
                         intervals = c(.5, .95), 
                         fill_by_model = TRUE, 
                         plot=FALSE,
                         plot_truth = FALSE,
                         facet = .~horizon_week,
                         fill_transparency = 0.6,
                         title = "none",
                         subtitle = "none",
                         show_caption = FALSE)


p_horizon <- p_horizon +
  scale_x_date(name=NULL, date_breaks = "1 weeks", date_labels = "%m-%d") +
  theme(axis.ticks.length.x = unit(0, "cm"),
        axis.text.x = element_blank(),
        legend.position = "none",
        axis.title.y = element_blank()) + 
  facet_grid(.~horizon_week, scales = "free_x")


# calculate CD for each horizon
cd_plot_frame <- read.csv("../data/quantile_frame_hosp_top.csv")
cd_plot_frame$target_end_date <- anydate(cd_plot_frame$target_end_date)
cd_plot_frame <- cd_plot_frame %>%  
  filter(model %in% c("CU-select","COVIDhub-ensemble"),
         location == 48,
         forecast_date <= as.Date("2021-01-28")) %>%
  frame_format2()

q_set <- unique(cd_plot_frame$quantile) 
cd_plot_list <- build_distance_frame(cd_plot_frame, 
                           horizon_list=c(1:4),
                           target_list="inc hosp",
                           approx_rule="trapezoid_riemann",
                           tau_F=q_set,tau_G=q_set)

cd_plot_cd <- cd_plot_list[[1]] %>%
  filter(approx_cd != 0) %>%
  distinct(approx_cd, .keep_all = TRUE) %>%
  select(horizon, approx_cd, target_end_date)

cd_plot_cd$target_end_date <- anydate(cd_plot_cd$target_end_date)
cd_plot_cd$approx_cd <- round(cd_plot_cd$approx_cd, 2)


p 
p_horizon 
```

```{r}
knitr::kable(select(cd_plot_cd, -target_end_date), 
             col.names = c("Horizon","Approx. CD"))
```

## Aligning Hospitalization Forecasts

Bracher et. al's work on model similarity specifically focused on incident case and death (inc case and inc death, respectively) forecasts, while this analysis is focusing on incident hospitalizations. Incident hospitalization (inc hosp) data has daily targets instead of weekly ones, like inc case and inc death data. This presents an issue with unaligned forecasts because each model only makes predictions on a single day per week but not all models make their forecasts on the same day. That is, forecasts made in the same week but on different days will be predicting for different target end dates, even if they share the horizon. (This is not an issue when the temporal resolution is in terms of weeks, which are defined by epidemiological week, not the number of days between forecast date and target end date.) Thus, we create a new relative horizon variable variable called horizon week to prevent unaligned forecasts. This variable counts horizons between 1 and 7 days to have a horizon week of 1, horizons between 8 and 14 days to have a horizon week of 2, etc. With this new variable, we can easily apply similar analyses from Bracher et. al to inc hosp data. 

One potential disadvantage lies in using the horizon week variable. Since models only make forecasts once per week, models that make forecasts later in the week may have an unfair advantage of additional days worth of data informing their forecasts. We will consider and investigate this issue later in the report.


## Forecast Inclusion Criteria

The pairwise approximated Cramér's distances are calculated for the models that have complete submissions for all targets, all probability levels, and no missing forecasts between January 28th, 2021 and June 10th 2021. We aggregate results for the five locations that have the highest number of cumulative COVID-19 hospitalizations during this period as well as the five locations with the lowest number, then perform the analysis on both "high count" and "low count" groupings. 

The high count locations are Florida, Texas, New York, California, Pennsylvania while the low count locations are Vermont, Hawaii, Arkansas, Wyoming, South Dakota. 

```{r read-in forecasts and truth, message=FALSE}
# read in forecasts
## high count - Thursday
wide_frame_hosp_high_thurs <- read.csv("../data/quantile_frame_hosp_top.csv") %>%
  dplyr::filter(!(model %in% c("CU-nochange","CU-scenario_high","CU-scenario_low","CU-scenario_mid")))
wide_frame_hosp_high_thurs$target_end_date <- anydate(wide_frame_hosp_high_thurs$target_end_date)

h_frame_high_thurs <- wide_frame_hosp_high_thurs %>% 
  frame_format2()

## low count - Thursday
wide_frame_hosp_low_thurs <- read.csv("../data/quantile_frame_hosp_bottom.csv") %>%
  dplyr::filter(!(model %in% c("CU-nochange","CU-scenario_high","CU-scenario_low","CU-scenario_mid")))
wide_frame_hosp_low_thurs$target_end_date <- anydate(wide_frame_hosp_low_thurs$target_end_date)

h_frame_low_thurs <- wide_frame_hosp_low_thurs %>% 
  frame_format2()


## high count - Saturday
wide_frame_hosp_high_sat <- read.csv("../data/quantile_frame_hosp_top_sat.csv") %>%
  dplyr::filter(!(model %in% c("CU-nochange","CU-scenario_high","CU-scenario_low","CU-scenario_mid")))
wide_frame_hosp_high_sat$target_end_date <- anydate(wide_frame_hosp_high_sat$target_end_date)

h_frame_high_sat <- wide_frame_hosp_high_sat %>% 
  frame_format2() %>%
  select(-c("LANL-GrowthRate")) # LANL doesn't meet criteria for Thursdays

## low count - Saturday
wide_frame_hosp_low_sat <- read.csv("../data/quantile_frame_hosp_bottom_sat.csv") %>%
  dplyr::filter(!(model %in% c("CU-nochange","CU-scenario_high","CU-scenario_low","CU-scenario_mid")))
wide_frame_hosp_low_sat$target_end_date <- anydate(wide_frame_hosp_low_sat$target_end_date)

h_frame_low_sat <- wide_frame_hosp_low_sat %>% 
  frame_format2() %>%
  select(-c("LANL-GrowthRate")) # LANL doesn't meet criteria for Thursdays



## truth data
filtered_start_date <- as.Date("2021-02-18")
end_date_sat <- as.Date("2021-06-12")

hosp_truth_high <- load_truth("HealthData", 
                         "inc hosp", 
                         temporal_resolution="weekly",
                         data_location = "remote_hub_repo")
hosp_truth_high <- hosp_truth_high %>%
  dplyr::filter(target_end_date >= as.Date("2021-01-28"),
                target_end_date <= end_date_sat,
                geo_type=="state",
                location_name %in% unique(wide_frame_hosp_high_thurs$location_name)) 

hosp_truth_low <- load_truth("HealthData", 
                         "inc hosp", 
                         temporal_resolution="weekly",
                         data_location = "remote_hub_repo") %>%
  dplyr::filter(target_end_date >= as.Date("2021-01-28"),
                target_end_date <= end_date_sat,
                geo_type=="state",
                location_name %in% unique(wide_frame_hosp_low_thurs$location_name)) 
```

There are nine models that fulfilled the criteria for both the five high count and five low count locations for Thursday forecasts. There are ten that fulfill such criteria for the Saturday forecasts, but we perform the analysis on only the overlapping nine models.


```{r metadata, warning=FALSE, message=FALSE}
# read in model metadata
metadata <- read.csv("../metadata_categorized.csv") 
metadata$stats[which(metadata$team_name == "Karlen Working Group")] <- TRUE
metadata$compartmental[which(metadata$team_name == "Robert Walraven")] <- FALSE
metadata$JHU_data[which(metadata$team_name == "COVID-19 Forecast Hub")] <- TRUE
metadata$ensemble <- ifelse(metadata$ensemble==TRUE,1,0)
metadata$compartmental <- ifelse(metadata$compartmental==TRUE,1,0)
metadata$stats <- ifelse(metadata$stats==TRUE,1,0)
# manual change 
# add text columns
metadata$model_type <- ifelse(metadata$ensemble, 
                                  "ensemble", 
                                  ifelse(metadata$stats + metadata$compartmental==2,
                                         "both stats and mech",
                                         ifelse((metadata$stats*2)+metadata$compartmental==2,
                                                "statistical",
                                                ifelse((metadata$stats*2)+metadata$compartmental==1,
                                                       "mechanistic",
                                                       "neither stats nor mech"))))
metadata$data_source <- ifelse(metadata$JHU_data,"JHU","unspecified")

h_meta <- colnames(h_frame_high_thurs)[-c(1:6)] #note the same 9 models for high and low counts

h_metadata <- metadata %>%
  dplyr::filter(model_abbr %in% h_meta)
recent_hmeta <- h_metadata %>%
  dplyr::group_by(team_name,model_name) %>%
  dplyr::filter(date==max(as.POSIXct(date))) %>%
  dplyr::ungroup() %>%
  dplyr::group_by(model_abbr) %>%
  dplyr::filter(date==max(as.POSIXct(date))) %>%
  dplyr::ungroup()
recent_hmeta[8,24] <- "mechanistic" # change the Karlen model to mech
recent_hmeta[5,6] <- "public mobility data, CSSE, HHS, CDC scenario 5 data" 
short_hmeta <- recent_hmeta[,c(3,ncol(recent_hmeta)-1)]
short_hdata <- recent_hmeta[,c(3,ncol(recent_hmeta))]

# model inputs
short_hinput <- select(recent_hmeta, c(model_abbr, data_inputs))

# model methods
h_methods <- select(recent_hmeta, c(model_abbr, methods, methods_long)) 
```


## Day of Week Effects

This analysis looks into whether the incorporation of day of the week effects impacts model similarity, e.g. do models that have day of the week effects more similar to each other than those without. We define day of the week effects to be cyclic weekly patterns for which a specific day or group of days specific show higher or lower incident hospitalization values compared to other days. The hospitalization truth data for high count locations clearly shows a day of the week effect in which weekends have noticeably fewer incident hospitalizations in comparison to weekdays. However, hospitalization truth data for low count locations is a little less clear. 

We plot point forecasts with a horizon week of 1 to determine which models include day of the week effects, first looking at high count locations, then low count locations. 

```{r day of wk effects high, fig.height=2.75, fig.align='center', message=FALSE, warning=FALSE, comment=FALSE}
# truth data
hosp_truth_high2 <- hosp_truth_high %>% 
  select(-c(location, 6:11)) %>%
  group_by(target_end_date) %>%
  mutate(agg_hosp = sum(value)) %>%
  distinct(agg_hosp, .keep_all = TRUE) %>%
  mutate(day_type = ifelse(wday(target_end_date) %in% 2:6, "weekday", "weekend"),
         recent_tues = floor_date(target_end_date, "weeks", 
                                 week_start = getOption("lubridate.week.start", 2)))

t_h <- ggplot(hosp_truth_high2, aes(x = target_end_date, y = agg_hosp)) + 
  geom_point(aes(color = day_type), size = 0.75) +
  geom_line(aes(group = recent_tues), size = 0.5) +
  labs(title = "Truth Data - High Count", 
       x = "target end date" , y= "inc hosp") +
  theme(legend.position = "bottom")

# forecasts
hosp_pt_high <- read.csv("../data/point_frame_hosp_top.csv")

hosp_pt_high$forecast_date <- anydate(hosp_pt_high$forecast_date)

hosp_pt_high <- hosp_pt_high %>%
  select(-c(quantile, location, 11:16)) %>%
  group_by(model, target_end_date, horizon_week, horizon) %>%
  mutate(agg_hosp = sum(value)) %>%
  distinct(agg_hosp, .keep_all = TRUE) %>%
  filter(horizon_week == 1, model %in% short_hdata$model_abbr) %>%
  mutate(day_type = ifelse(wday(target_end_date) %in% 2:6, "weekday", "weekend")) %>%
  mutate(forecast_day = weekdays(forecast_date))

hosp_pt_high$target_end_date <- anydate(hosp_pt_high$target_end_date)
hosp_pt_high$day_type <- as.factor(hosp_pt_high$day_type)

p1_h <- plot_day_of_week_effect_color(hosp_pt_high, "COVIDhub-ensemble") +
  theme(legend.position = "bottom")
p2_h <- plot_day_of_week_effect_color(hosp_pt_high, "JHUAPL-Bucky") # as.Date("2021-02-27")
p3_h <- plot_day_of_week_effect_color(hosp_pt_high, "JHUAPL-Gecko")

p4_h <- plot_day_of_week_effect_color(hosp_pt_high, "Google_Harvard-CPF") 
p5_h <- plot_day_of_week_effect_color(hosp_pt_high, "Covid19Sim-Simulator")
p6_h <- plot_day_of_week_effect_color(hosp_pt_high, "CU-select")
p7_h <- plot_day_of_week_effect_color(hosp_pt_high, "Karlen-pypm") 
p8_h <- plot_day_of_week_effect_color(hosp_pt_high, "JHUAPL-SLPHospEns")
p9_h <- plot_day_of_week_effect_color(hosp_pt_high, "MOBS-GLEAM_COVID")

t_h + p1_h + plot_layout()

```

```{r, out.width="97%"}
# new code chunk to have default plot size
p2_h + p3_h + p4_h + p5_h + plot_layout(ncol = 2)
p6_h + p7_h + p8_h + p9_h + plot_layout(ncol = 2)
```

We can see that only two models, JHUAPL-Bucky and JHUAPL-Gecko, incorporate day of week effects based on the high count data.  

```{r day of wk effects low, fig.height=2.75, fig.align='center', message=FALSE, warning=FALSE, comment=FALSE}
# truth data
hosp_truth_low2 <- hosp_truth_low %>% 
 # select(-c(location, 6:11)) %>%
  group_by(target_end_date) %>%
  mutate(agg_hosp = sum(value)) %>%
  distinct(agg_hosp, .keep_all = TRUE) %>%
  mutate(day_type = ifelse(wday(target_end_date) %in% 2:6, "weekday", "weekend"),
         recent_tues = floor_date(target_end_date, "weeks", 
                                 week_start = getOption("lubridate.week.start", 2)))

t_l <- ggplot(hosp_truth_low2, aes(x = target_end_date, y = agg_hosp)) + 
  geom_point(aes(color = day_type)) +
  geom_line(aes(group = recent_tues)) +
  labs(title = "Truth Data - Low Count", 
       x = "target end date" , y= "inc hosp") +
  theme(legend.position = "bottom")

# foreasts
hosp_pt_low <- read.csv("../data/point_frame_hosp_bottom.csv")

hosp_pt_low <- hosp_pt_low %>%
  select(-c(quantile, location, 11:16)) %>%
  group_by(model, target_end_date, horizon_week) %>%
  mutate(agg_hosp = sum(value)) %>%
  distinct(agg_hosp, .keep_all = TRUE) %>%
  filter(horizon_week == 1) %>%
  mutate(day_type = ifelse(wday(target_end_date) %in% 2:6, "weekday", "weekend"))

hosp_pt_low$target_end_date <- anydate(hosp_pt_low$target_end_date)
hosp_pt_low$day_type <- as.factor(hosp_pt_low$day_type)

p1_l <- plot_day_of_week_effect_color(hosp_pt_low, "COVIDhub-ensemble") # as.Date("2021-02-13")
p2_l <- plot_day_of_week_effect_color(hosp_pt_low, "JHUAPL-Bucky") # as.Date("2021-02-27")
p3_l <- plot_day_of_week_effect_color(hosp_pt_low, "JHUAPL-Gecko")
p4_l <- plot_day_of_week_effect_color(hosp_pt_low, "Google_Harvard-CPF") 

p5_l <- plot_day_of_week_effect_color(hosp_pt_low, "Covid19Sim-Simulator")
p6_l <- plot_day_of_week_effect_color(hosp_pt_low, "CU-select")
p7_l <- plot_day_of_week_effect_color(hosp_pt_low, "Karlen-pypm") 
p8_l <- plot_day_of_week_effect_color(hosp_pt_low, "JHUAPL-SLPHospEns")
p9_l <- plot_day_of_week_effect_color(hosp_pt_low, "MOBS-GLEAM_COVID")


t_l + p1_l + plot_layout()
```

```{r, out.width="97%"}
# new code chunk to have default plot size
p2_l + p3_l + p4_l + p5_l + plot_layout(ncol = 2)
p6_l + p7_l + p8_l + p9_l + plot_layout(ncol = 2)
```

We reiterate that low count location hospitalization truth data doesn't show as clear day of the week effects as high count truth data. However, we can still see that the same two models, JHUAPL-Bucky and JHUAPL-Gecko, do show day of the week effects for low count locations. Thus, we can categorize models into three categories: true (shows a day of the week effect), false (does not show a day of the week effect), or ensemble (a model built from other models that may or may not include day of the week effects). We create a third ensemble category because ensembles themselves do not incorporate day of the week effects, as they are a collection of other models, but the included models may or may not incorporate day of the week effects.

This categorization is shown in the table below. 

```{r day of week effects metadata}
recent_hmeta$day_of_wk_effect <- 
  as.factor(c("FALSE", "ENS", "FALSE", "FALSE", "TRUE", "TRUE", "ENS", "FALSE", "FALSE"))

short_hday <- recent_hmeta %>% 
  select(c(model_abbr, day_of_wk_effect))
  

knitr::kable(arrange(short_hday, day_of_wk_effect), col.names = c("Model","Day of Week Effect"))

metadata <- metadata %>%
  left_join(select(recent_hmeta, c(model_name, day_of_wk_effect)), by = c("model_name"))
```


Below we investigate the issue of different forecast dates making the horizon week variable not an entirely fair metric for the models. As it turns out, however, this is not a major problem because all of the models make their forecasts on either Sunday or Monday (some models have switched during the period of interest). The following table summarizes which day(s) each of the nine models have made their forecasts on. 

```{r forecast date metadata}
short_hforecast_day <- hosp_pt_high %>%
  group_by(model) %>%
  distinct(forecast_day) %>%
  filter(model %in% short_hmeta$model_abbr) %>%
  distinct(model,.keep_all = TRUE) %>%
  arrange(model)

short_hforecast_day[4,2] <- "Sunday, Monday"
short_hforecast_day[6,2] <- "Sunday, Monday"

  # mobs was on sun instead of monday for forecasts made on 5/02 
  # gecko was Sun on 1/24, 3/14, 4/04 - 6/06 ; Mon on 2/01 - 3/08, 3/22, 3/29
  # google was Mon 1/25, 2/01, 2/15, 2/22, 3/08, 3/29 - 5/10, 5/24; 
  #            Sun 2/07, 2/28, 3/14, 3/21, 5/16, 5/30, 6/06

knitr::kable(short_hforecast_day, col.names = c("Model","Forecast Day"))
```


Given that hospitalization forecasts are daily, there are seven times as many forecasts made for incident hospitalizations as compared to incident cases and incident deaths. Because we are looking at day of the week effects, we choose to focus on only two days of the week for our analysis, one weekday and one weekend (Thursday and Saturday, respectively). This way, we can investigate if there seems to be a meaningful difference between model similarity on a weekday versus on a weekend.


## Weekday Analysis
This portion of the analysis only examines target end dates for a single weekday, Thursday, to account for models that include day of the week effects. 

```{r build data and figures thurs}
# calculate distance matrices
q_set_hosp_thurs <- unique(h_frame_high_thurs$quantile) 
approx_cd_list_high_thurs <- build_distance_frame(h_frame_high_thurs, 
                           horizon_list=c(1:4),
                           target_list="inc hosp",
                           approx_rule="trapezoid_riemann",
                           tau_F=q_set_hosp_thurs,tau_G=q_set_hosp_thurs)

approx_cd_list2_high <- suppressWarnings(build_distance_frame(h_frame_high_thurs, 
                                        horizon_list=c(1:4),
                                        target_list="inc hosp",
                                        approx_rule="approximation2",
                                        tau_F=q_set_hosp_thurs,tau_G=q_set_hosp_thurs))

  
# extract data
total_frame_high_thurs <- approx_cd_list_high_thurs[[1]] %>%
  dplyr::mutate(pair=paste(model_1,model_2,sep=" vs ")) %>%
  dplyr::group_by(horizon,target_variable,target_end_date,pair) %>%
  dplyr::mutate(mean_approx_cd=mean(approx_cd)) %>% #mean cd across all locations
  dplyr::ungroup() %>%
  dplyr::select(-c("approx_cd","location")) %>%
  dplyr::distinct()
total_frame_high_thurs$target_end_date <- 
  as.Date(total_frame_high_thurs$target_end_date,origin="1980-01-01")

h_frame_mean_high_thurs <- lapply(1:4, function(x) cd_matrix(approx_cd_list_high_thurs[[3]],x))


# make data for box plot
newdf_high <- approx_cd_list_high_thurs[[3]][, c(1:3)] %>%
  rowwise() %>%
  dplyr::mutate(h=ifelse(horizon==1,"a",ifelse(horizon==2,"b",ifelse(horizon==3,"c","d")))) %>%
  dplyr::select(-"horizon")
for (i in 1:nrow(approx_cd_list_high_thurs[[3]])){
    newdf_high[i, ] = sort(newdf_high[i,c(1:3)])
}

pair_data_high <- approx_cd_list_high_thurs[[3]] %>%
  dplyr::left_join(short_hmeta,by=c("model_1"="model_abbr")) %>%
  dplyr::left_join(short_hmeta,by=c("model_2"="model_abbr")) %>%
  dplyr::rename(model1_type=model_type.x,
                model2_type=model_type.y) %>%
  rowwise() %>%
  dplyr::mutate(stats_type=ifelse((model1_type== "statistical"&&model2_type== "statistical"),
                                  "both statistical",
                                  ifelse((model1_type== "statistical"|model2_type== "statistical"),
                                  "one is statistical", "both not statistical")),
                mech_type=ifelse((model1_type== "mechanistic" && model2_type== "mechanistic"),
                                  "both mechanistic",
                                  ifelse((model1_type== "mechanistic"|model2_type== "mechanistic"),
                                  "one is mechanistic", "both not mechanistic"))) %>%
  .[!duplicated(newdf_high),] %>%
  dplyr::filter(model_1!=model_2)


# low count locations
approx_cd_list_low_thurs <- build_distance_frame(h_frame_low_thurs, 
                           horizon_list=c(1:4),
                           target_list="inc hosp",
                           approx_rule="trapezoid_riemann",
                           tau_F=q_set_hosp_thurs,tau_G=q_set_hosp_thurs)

# approx_cd_list2_low <- build_distance_frame(d_frame_low, 
#                                         horizon_list=c(1:4),
#                                         target_list="inc death",
#                                         approx_rule="approximation2",
#                                         tau_F=q_set,tau_G=q_set)
# extract data
total_frame_low_thurs <- approx_cd_list_low_thurs[[1]] %>%
  dplyr::mutate(pair=paste(model_1,model_2,sep=" vs ")) %>%
  dplyr::group_by(horizon,target_variable,target_end_date,pair) %>%
  dplyr::mutate(mean_approx_cd=mean(approx_cd)) %>%
  dplyr::ungroup() %>%
  dplyr::select(-c("approx_cd","location")) %>%
  dplyr::distinct()
total_frame_low_thurs$target_end_date <- as.Date(total_frame_low_thurs$target_end_date,origin="1970-01-01")

h_frame_mean_low_thurs <- lapply(1:4, function(x) cd_matrix(approx_cd_list_low_thurs[[3]],x))

newdf_low <- approx_cd_list_low_thurs[[3]][,c(1:3)] %>%
  rowwise() %>%
  dplyr::mutate(h=ifelse(horizon==1,"a",ifelse(horizon==2,"b",ifelse(horizon==3,"c","d")))) %>%
  dplyr::select(-"horizon")
for (i in 1:nrow(approx_cd_list_low_thurs[[3]])){
    newdf_low[i, ] = sort(newdf_low[i,c(1:3)])
}
pair_data_low <- approx_cd_list_low_thurs[[3]] %>%
  dplyr::left_join(short_hmeta,by=c("model_1"="model_abbr")) %>%
  dplyr::left_join(short_hmeta,by=c("model_2"="model_abbr")) %>%
  dplyr::rename(model1_type=model_type.x,
                model2_type=model_type.y) %>%
  rowwise() %>%
  dplyr::mutate(stats_type=ifelse((model1_type== "statistical"&&model2_type== "statistical"),
                                  "both statistical",
                                  ifelse((model1_type== "statistical"|model2_type== "statistical"),
                                  "one is statistical", "both not statistical")),
                mech_type=ifelse((model1_type== "mechanistic" && model2_type== "mechanistic"),
                                  "both mechanistic",
                                  ifelse((model1_type== "mechanistic"|model2_type== "mechanistic"),
                                  "one is mechanistic", "both not mechanistic"))) %>%
  .[!duplicated(newdf_low),] %>%
  dplyr::filter(model_1!=model_2)
```


First, we visualize the mean approximated pairwise Cramér's Distance across the entire period of interest in the heatmaps shown below. The distance from the model to itself is zero. The $x-$axis is arranged based on the categorization of models outlined above: blue is incorporates a day of week effect, red is does not incorporate a day of the week effect, and purple is ensemble model. 

```{r heatmaps thurs,out.width="95%", fig.align='center', warning=FALSE}
distance_heatmap_wk(approx_cd_list_high_thurs[[3]],
                "Mean Approx. CD of Inc Hosp Forecasts by Horizon - High Count Locations", recent_hmeta) 

distance_heatmap_wk(approx_cd_list_low_thurs[[3]],
                "Mean Approx. CD of Inc Hosp Forecasts by Horizon - Low Count Locations", recent_hmeta)
```

Google_Harvard-CPF is generally the least similar to the other models for both the high count and low count locations. This is true across all horizons for the high count locations, but only true for the one and two week horizons of the low count locations. Covid19Sim-Simulator, JHUAPL-Bucky, and JHUAPL-Gecko show similar Cramér's Distances at three and four week horizons at the low count locations. However, it is important to note the small scale observed for the low count locations may explain why three models have such similar Cramér's Distances.

Since Google_Harvard-CPF is substantially dissimilar to the other models, especially for high count locations, it obscures any potential patterns about day of the week effect and model similarity that we might find among the other models. Thus, we plot the heat maps again, this time excluding Google_Harvard-CPF. 

```{r heatmaps thurs no Google Harvard,out.width="95%", fig.align='center', warning=FALSE}
distance_heatmap_wk(filter(approx_cd_list_high_thurs[[3]], 
                           model_1 != "Google_Harvard-CPF", model_2 != "Google_Harvard-CPF"),
                "Mean Approx. CD of Inc Hosp Forecasts by Horizon - High Count Locations", recent_hmeta)
distance_heatmap_wk(filter(approx_cd_list_low_thurs[[3]], 
                           model_1 != "Google_Harvard-CPF", model_2 != "Google_Harvard-CPF"),
                "Mean Approx. CD of Inc Hosp Forecasts by Horizon - Low Count Locations", recent_hmeta)
```

We can see that for both high and low count locations, at all horizons, Covid19Sim-Simulator becomes the most dissimilar model without Google_Harvard-CPF present. JHUAPL-Bucky seems to have the next highest Cramér's Distances at 1 to 3 week horizons, but at a 4-week horizon, the models without a day of the week effect show higher Cramér's Distances. Low count locations show that models with day of the week effect have greater dissimilarity than models without, except for Covid19Sim-Simulator. However, the low count location results can be rather sensitive to small variations due to chance alone given their small Cramér's Distance values.


We can also look at the approximated pairwise distances over time to see how the models become more similar or dissimilar at different points during the period of interest.

```{r,out.width="95%", fig.align='center'}
ot_data_high_thurs <- total_frame_high_thurs %>% 
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="COVIDhub-ensemble",
                model_2 != "COVIDhub-ensemble") %>%
  dplyr:: ungroup() 

ot_data_low_thurs <- total_frame_low_thurs %>% 
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="COVIDhub-ensemble",
                model_2 != "COVIDhub-ensemble") %>%
  dplyr:: ungroup()


scatter_wk(ot_data_high_thurs,
        "Mean Approx. CD from COVIDhub-ensemble Over Time - \nThursday, High Count Locations",
        recent_hmeta,
        smooth_tf = TRUE) 
scatter_wk(ot_data_low_thurs,
        "Mean Approx. CD from COVIDhub-ensemble Over Time - \nThursday, Low Count Locations",
        recent_hmeta,
        smooth_tf = TRUE) 
```

The scatterplots show that the Google_Harvard-CPF, Covid19Sim-Simulator, and JHUAPL-Bucky models tend to differ from the Covidhub-ensemble model compared to the other models. This seems to align with the results shown in the heat maps above that show that Google_Harvard-CPF, Covid19Sim-Simulator, and JHUAPL-Bucky tend to have the highest mean Cramér's Distance from the other models. In high count locations, Google_Harvard-CPF is substantially different from the ensemble model during the entire period of interest. However, in low count locations, JHUAPL-Bucky shows a peak in March, although this peak is actually rather small, given the scale. 

Whether models incorporate a day of the week effect does not seem to have an impact on how much the model differs from the ensemble, nor as to when it differs greatly.

```{r truth over time, fig.height = 3.5, fig.align='center', warning=FALSE, message=FALSE}
# plot truth data
tr_h <- ggplot(rename(filter(hosp_truth_high, target_end_date >= filtered_start_date), 
                      Location = `abbreviation`), 
               aes(x = target_end_date, y = value, color = Location)) + 
      geom_point(alpha=0.6,size=0.8) + 
      stat_smooth(alpha=0.4,size=0.5,aes(x = target_end_date, y = value), method = "loess",
                  formula = y ~ x, se = FALSE) +
      ggtitle("Hosp Truth Over Time - \nHigh Count Locations") +
      ylab("Inc Hosp") +
      xlab("Forecast End Date") +
      theme(legend.text = element_text(size=5),
            legend.title = element_text(size=9),
            axis.text.x=element_text(size=rel(0.7),angle=45,hjust=1),
            legend.key.size = unit(0.5, 'cm'),
            legend.position = "bottom")+
      scale_x_date(date_breaks = "1 month",
                   date_labels = "%m-%y")

tr_l <- ggplot(rename(filter(hosp_truth_high, target_end_date >= filtered_start_date), 
                      Location = `abbreviation`), 
               aes(x = target_end_date, y = value, color = Location)) + 
      geom_point(alpha=0.6,size=0.8) + 
      stat_smooth(alpha=0.4,size=0.5,aes(x = target_end_date, y = value), method = "loess",
                  formula = y ~ x, se = FALSE) +
      ggtitle("Hosp Truth Over Time - \nLow Count Locations") +
      xlab("Forecast End Date") +
      theme(legend.text = element_text(size=5),
            legend.title = element_blank(),
            axis.text.x=element_text(size=rel(0.7),angle=45,hjust=1),
            axis.title.y = element_blank(),
            legend.key.size = unit(0.5, 'cm'),
            legend.position = "bottom")+
      scale_x_date(date_breaks = "1 month",
                   date_labels = "%m-%y")

tr_h + tr_l + plot_layout(ncol = 2)
```

It seems that Google_Harvard-CPF and Covid19Sim-Simulator's differences from the ensemble model follow the trends shown by the truth data. 


We can also create dendrograms from the distances using hierarchical clustering. 

```{r high dendro thurs,out.width="90%", fig.cap="High Hospitalization Count Locations", fig.align='center', message=FALSE, warning=FALSE}
for(i in 1:4){
    assign(paste0("dp_t",i),
           dendro_plot_wk(i, "h_frame_mean_high_thurs",short_hday)
             )
    assign(paste0("dpl_t",i),
           dendro_plot_wk(i, "h_frame_mean_low_thurs",short_hday)
             )
}

dp_t1 <- dp_t1 +
  geom_rect(aes(xmin = 0.7, ymin = -25, xmax = 1.3, ymax = 110), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 1.7, ymin = -25, xmax = 2.3, ymax = 110), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 2.7, ymin = -25, xmax = 9.3, ymax = 110), 
            fill = NA, color = "black", linetype = "dashed")

dp_t2 <- dp_t2 +
  geom_rect(aes(xmin = 0.7, ymin = -25, xmax = 1.3, ymax = 110), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 1.7, ymin = -25, xmax = 2.3, ymax = 110), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 2.7, ymin = -25, xmax = 9.3, ymax = 110), 
            fill = NA, color = "black", linetype = "dashed")

dp_t3 <- dp_t3 +
  geom_rect(aes(xmin = 0.7, ymin = -25, xmax = 1.3, ymax = 125), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 1.7, ymin = -25, xmax = 2.3, ymax = 125), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 2.7, ymin = -25, xmax = 9.3, ymax = 125), 
            fill = NA, color = "black", linetype = "dashed")

dp_t4 <- dp_t4 +
  geom_rect(aes(xmin = 0.7, ymin = -25, xmax = 1.3, ymax = 170), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 1.7, ymin = -25, xmax = 2.3, ymax = 170), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 2.7, ymin = -25, xmax = 9.3, ymax = 170), 
            fill = NA, color = "black", linetype = "dashed")

grid.arrange(dp_t1,dp_t2,dp_t3,dp_t4)
```

```{r low dendro thurs,out.width="90%", fig.align='center',fig.cap="Low Hospitalization Count Locations"}
dpl_t1 <- dpl_t1 +
  geom_rect(aes(xmin = 0.7, ymin = -0.25, xmax = 1.3, ymax = 1.9), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 1.7, ymin = -0.25, xmax = 2.3, ymax = 1.9), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 2.7, ymin = -0.25, xmax = 9.3, ymax = 1.9), 
            fill = NA, color = "black", linetype = "dashed")

dpl_t2 <- dpl_t2 +
  geom_rect(aes(xmin = 0.7, ymin = -0.25, xmax = 7.3, ymax = 2.2), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 7.7, ymin = -0.25, xmax = 8.3, ymax = 2.2), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 8.7, ymin = -0.25, xmax = 9.3, ymax = 2.2), 
            fill = NA, color = "black", linetype = "dashed")

dpl_t3 <- dpl_t3 +
  geom_rect(aes(xmin = 0.7, ymin = -0.25, xmax = 7.3, ymax = 2.45), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 7.7, ymin = -0.25, xmax = 8.3, ymax = 2.45), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 8.7, ymin = -0.25, xmax = 9.3, ymax = 2.45), 
            fill = NA, color = "black", linetype = "dashed")

dpl_t4 <- dpl_t4 +
  geom_rect(aes(xmin = 0.7, ymin = -0.25, xmax = 1.3, ymax = 2.8), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 1.7, ymin = -0.25, xmax = 2.3, ymax = 2.8), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 2.7, ymin = -0.25, xmax = 9.3, ymax = 2.8), 
            fill = NA, color = "black", linetype = "dashed")

grid.arrange(dpl_t1,dpl_t2,dpl_t3,dpl_t4,nrow=2)
```

For each dendrogram, we split the models into three groups. Most high count and low count dendrograms include two single-model groups that consist of a model without day of the week effects.  However, this does not seem to be indicative of any trends in model similarity related to day of the week effect, more just that Google_Harvard-CPF and Covid19Sim-Simulator tend to be more different from the other models over all. If we were to create the dendrograms without Google_Harvard-CPF, then JHUAPL-Bucky, a model that includes day of the week effects, would become its own group. The low count 3-week horizon dendrogram split into four groups more easily than three groups; however, the scale for differences in Cramér Distance for the low count locations is very small, likely a result of low incident hospitalizations, which may this difference. 


Overall, it seems that Google_Harvard-CPF is consistently the most dissimilar from the other models by a substantial amount, followed by Covid19Sim-Simulator, across almost all horizons for both high-count and low count regions. For Thursday forecasts, there is not a clear conclusion to be drawn about the impact of day of week effects on model similarity. 



## Weekend Analysis

Now we look at forecasts with a target end date on a Saturday to see if day of the week effects change model similarity. We expect forecasts for weekends to show the impact of day of the week effects more strongly than weekdays. 

```{r build data and figures sat}
# calculate distance matrices
q_set_hosp_sat <- unique(h_frame_high_sat$quantile) 
approx_cd_list_high_sat <- build_distance_frame(h_frame_high_sat, 
                           horizon_list=c(1:4),
                           target_list="inc hosp",
                           approx_rule="trapezoid_riemann",
                           tau_F=q_set_hosp_sat,tau_G=q_set_hosp_sat)

approx_cd_list2_high <- suppressWarnings(build_distance_frame(h_frame_high_sat, 
                                        horizon_list=c(1:4),
                                        target_list="inc hosp",
                                        approx_rule="approximation2",
                                        tau_F=q_set_hosp_sat,tau_G=q_set_hosp_sat))

  
# extract data
total_frame_high_sat <- approx_cd_list_high_sat[[1]] %>%
  dplyr::mutate(pair=paste(model_1,model_2,sep=" vs ")) %>%
  dplyr::group_by(horizon,target_variable,target_end_date,pair) %>%
  dplyr::mutate(mean_approx_cd=mean(approx_cd)) %>% #mean cd across all locations
  dplyr::ungroup() %>%
  dplyr::select(-c("approx_cd","location")) %>%
  dplyr::distinct()
total_frame_high_sat$target_end_date <- as.Date(total_frame_high_sat$target_end_date,origin="1980-01-01")

h_frame_mean_high_sat <- lapply(1:4, function(x) cd_matrix(approx_cd_list_high_sat[[3]],x))


# make data for box plot
newdf_high <- approx_cd_list_high_sat[[3]][, c(1:3)] %>%
  rowwise() %>%
  dplyr::mutate(h=ifelse(horizon==1,"a",ifelse(horizon==2,"b",ifelse(horizon==3,"c","d")))) %>%
  dplyr::select(-"horizon")
for (i in 1:nrow(approx_cd_list_high_sat[[3]])){
    newdf_high[i, ] = sort(newdf_high[i,c(1:3)])
}

pair_data_high <- approx_cd_list_high_sat[[3]] %>%
  dplyr::left_join(short_hmeta,by=c("model_1"="model_abbr")) %>%
  dplyr::left_join(short_hmeta,by=c("model_2"="model_abbr")) %>%
  dplyr::rename(model1_type=model_type.x,
                model2_type=model_type.y) %>%
  rowwise() %>%
  dplyr::mutate(stats_type=ifelse((model1_type== "statistical"&&model2_type== "statistical"),
                                  "both statistical",
                                  ifelse((model1_type== "statistical"|model2_type== "statistical"),
                                  "one is statistical", "both not statistical")),
                mech_type=ifelse((model1_type== "mechanistic" && model2_type== "mechanistic"),
                                  "both mechanistic",
                                  ifelse((model1_type== "mechanistic"|model2_type== "mechanistic"),
                                  "one is mechanistic", "both not mechanistic"))) %>%
  .[!duplicated(newdf_high),] %>%
  dplyr::filter(model_1!=model_2)


# low count locations
approx_cd_list_low_sat <- build_distance_frame(h_frame_low_sat, 
                           horizon_list=c(1:4),
                           target_list="inc hosp",
                           approx_rule="trapezoid_riemann",
                           tau_F=q_set_hosp_sat,tau_G=q_set_hosp_sat)

# approx_cd_list2_low <- build_distance_frame(d_frame_low, 
#                                         horizon_list=c(1:4),
#                                         target_list="inc death",
#                                         approx_rule="approximation2",
#                                         tau_F=q_set,tau_G=q_set)
# extract data
total_frame_low_sat <- approx_cd_list_low_sat[[1]] %>%
  dplyr::mutate(pair=paste(model_1,model_2,sep=" vs ")) %>%
  dplyr::group_by(horizon,target_variable,target_end_date,pair) %>%
  dplyr::mutate(mean_approx_cd=mean(approx_cd)) %>%
  dplyr::ungroup() %>%
  dplyr::select(-c("approx_cd","location")) %>%
  dplyr::distinct()
total_frame_low_sat$target_end_date <- as.Date(total_frame_low_sat$target_end_date,origin="1970-01-01")

h_frame_mean_low_sat <- lapply(1:4, function(x) cd_matrix(approx_cd_list_low_sat[[3]],x))

newdf_low <- approx_cd_list_low_sat[[3]][,c(1:3)] %>%
  rowwise() %>%
  dplyr::mutate(h=ifelse(horizon==1,"a",ifelse(horizon==2,"b",ifelse(horizon==3,"c","d")))) %>%
  dplyr::select(-"horizon")
for (i in 1:nrow(approx_cd_list_low_sat[[3]])){
    newdf_low[i, ] = sort(newdf_low[i,c(1:3)])
}
pair_data_low <- approx_cd_list_low_sat[[3]] %>%
  dplyr::left_join(short_hmeta,by=c("model_1"="model_abbr")) %>%
  dplyr::left_join(short_hmeta,by=c("model_2"="model_abbr")) %>%
  dplyr::rename(model1_type=model_type.x,
                model2_type=model_type.y) %>%
  rowwise() %>%
  dplyr::mutate(stats_type=ifelse((model1_type== "statistical"&&model2_type== "statistical"),
                                  "both statistical",
                                  ifelse((model1_type== "statistical"|model2_type== "statistical"),
                                  "one is statistical", "both not statistical")),
                mech_type=ifelse((model1_type== "mechanistic" && model2_type== "mechanistic"),
                                  "both mechanistic",
                                  ifelse((model1_type== "mechanistic"|model2_type== "mechanistic"),
                                  "one is mechanistic", "both not mechanistic"))) %>%
  .[!duplicated(newdf_low),] %>%
  dplyr::filter(model_1!=model_2)
```


We again visualize the mean approximated pairwise Cramér's Distance across the entire period of interest in the heatmaps shown below, this time for Saturday forecasts. The distance from the model to itself is zero. The $x-$axis is arranged based on the categorization of models outlined above: blue is incorporates a day of week effect, red is does not incorporate a day of the week effect, and purple is ensemble model. 

```{r heatmaps sat,out.width="95%", fig.align='center', warning=FALSE}
distance_heatmap_wk(approx_cd_list_high_sat[[3]],
                "Mean Approx. CD of Inc Hosp Forecasts by Horizon - High Count Locations", 
                recent_hmeta) 

distance_heatmap_wk(approx_cd_list_low_sat[[3]],
                "Mean Approx. CD of Inc Hosp Forecasts by Horizon - Low Count Locations", 
                recent_hmeta)
```

Similarly to the Thursday forecasts, Google_Harvard-CPF is generally the least similar to the other models for both the high count and low count locations. This is true across all horizons for the high count locations, but only true for the one and two week horizons of the low count locations. Covid19Sim-Simulator is the most dissimilar for the three and four week horizons of the low count locations. However, the small scale observed for the low count locations may explain why this model has a higher Cramér's Distance at longer horizons rather than a true pattern.

Like before, Google_Harvard-CPF is substantially dissimilar to the other models, especially for high count locations, it obscures any potential patterns about day of the week effect and model similarity that we might find among the other models. Thus, we plot the heat maps again, this time excluding Google_Harvard-CPF. 

```{r, out.width="95%", fig.align='center', message=FALSE, warning=FALSE, comment=FALSE}
distance_heatmap_wk(filter(approx_cd_list_high_sat[[3]], 
                           model_1 != "Google_Harvard-CPF", model_2 != "Google_Harvard-CPF"),
                "Mean Approx. CD of Inc Hosp Forecasts by Horizon - High Count Locations", 
                recent_hmeta)
distance_heatmap_wk(filter(approx_cd_list_low_sat[[3]], 
                           model_1 != "Google_Harvard-CPF", model_2 != "Google_Harvard-CPF"),
                "Mean Approx. CD of Inc Hosp Forecasts by Horizon - Low Count Locations", 
                recent_hmeta)
```
Without Google_Harvard-CPF present, Covid19Sim-Simulator becomes the most dissimilar model for both high and low count locations, at all horizons. JHUAPL-Bucky seems to have the next highest Cramér's Distances at 1 to 3 week horizons, but at a 4-week horizon, the models without a day of the week effect show higher Cramér's Distances. Low count locations show that models with day of the week effect have greater dissimilarity than models without, save for Covid19Sim-Simulator. However, the low count location results can be rather sensitive to small variations due to chance alone given their small Cramér's Distance values.


We can also look at the approximated pairwise distances to see how the models become more similar or dissimilar over time.

```{r cd over time,out.width="95%", fig.align='center'}
ot_data_high_sat <- total_frame_high_sat %>% 
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="COVIDhub-ensemble",
                model_2 != "COVIDhub-ensemble") %>%
  dplyr:: ungroup() 

ot_data_low_sat <- total_frame_low_sat %>% 
  dplyr::group_by(horizon,target_end_date) %>%
  dplyr::filter(model_1 =="COVIDhub-ensemble",
                model_2 != "COVIDhub-ensemble") %>%
  dplyr:: ungroup()


scatter_wk(ot_data_high_sat,
        "Mean Approx. CD from COVIDhub-ensemble Over Time - \nHigh Count Locations",
        recent_hmeta,
        smooth_tf = TRUE) 
scatter_wk(ot_data_low_sat,
        "Mean Approx. CD from COVIDhub-ensemble Over Time - \nLow Count Locations",
        recent_hmeta,
        smooth_tf = TRUE) 
```

The scatterplots for Saturday forecasts closely resemble those of the Thursday forecasts. Google_Harvard-CPF, Covid19Sim-Simulator, and JHUAPL-Bucky models generally differ from the Covidhub-ensemble model, which aligns with the results from the heat maps above: Google_Harvard-CPF, Covid19Sim-Simulator, and JHUAPL-Bucky tend to have the highest mean Cramér's Distance from the other models. In high count locations, Google_Harvard-CPF is substantially different from the ensemble model during the entire period of interest. However, in low count locations, JHUAPL-Bucky shows a peak in March, although this peak is actually rather small, given the scale.

Whether models incorporate a day of the week effect does not seem to have an impact on how much the model differs from the ensemble, nor as to when it differs greatly.

These scatterplots are nearly the same as the ones shown above. 


```{r truth over time2,fig.height = 3.5, out.width="95%", fig.align='center', warning=FALSE, message=FALSE}
# plot truth data
tr_h + tr_l + plot_layout(ncol = 2)
```

Like with the Thursday forecasts, it seems that Google_Harvard-CPF and Covid19Sim-Simulator's differences from the ensemble model follow the trends shown by the truth data. 


We can also cluster the distances using hierarchical clustering. 

```{r high dendro sat,out.width="90%", fig.cap="High Hospitalization Count Locations", fig.align='center', message=FALSE, warning=FALSE}
for(i in 1:4){
    assign(paste0("dp_s",i),
           dendro_plot_wk(i, "h_frame_mean_high_sat",short_hday)
             )
    assign(paste0("dpl_s",i),
           dendro_plot_wk(i, "h_frame_mean_low_sat",short_hday)
             )
}

dp_s1 <- dp_s1 +
  geom_rect(aes(xmin = 0.7, ymin = -25, xmax = 1.3, ymax = 100), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 1.7, ymin = -25, xmax = 2.3, ymax = 100), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 2.7, ymin = -25, xmax = 9.3, ymax = 100), 
            fill = NA, color = "black", linetype = "dashed")

dp_s2 <- dp_s2 +
  geom_rect(aes(xmin = 0.7, ymin = -25, xmax = 1.3, ymax = 110), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 1.7, ymin = -25, xmax = 2.3, ymax = 110), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 2.7, ymin = -25, xmax = 9.3, ymax = 110), 
            fill = NA, color = "black", linetype = "dashed")

dp_s3 <- dp_s3 +
  geom_rect(aes(xmin = 0.7, ymin = -25, xmax = 1.3, ymax = 125), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 1.7, ymin = -25, xmax = 2.3, ymax = 125), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 2.7, ymin = -25, xmax = 9.3, ymax = 125), 
            fill = NA, color = "black", linetype = "dashed")

dp_s4 <- dp_s4 +
  geom_rect(aes(xmin = 0.7, ymin = -25, xmax = 1.3, ymax = 170), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 1.7, ymin = -25, xmax = 7.3, ymax = 170), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 7.7, ymin = -25, xmax = 9.3, ymax = 170), 
            fill = NA, color = "black", linetype = "dashed")

grid.arrange(dp_s1,dp_s2,dp_s3,dp_s4)
```

```{r low dendro sat,out.width="90%", fig.align='center',fig.cap="Low Hospitalization Count Locations",fig.height=4}
dpl_s1 <- dpl_s1 +
  geom_rect(aes(xmin = 0.7, ymin = -0.25, xmax = 1.3, ymax = 2), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 1.7, ymin = -0.25, xmax = 2.3, ymax = 2), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 2.7, ymin = -0.25, xmax = 9.3, ymax = 2), 
            fill = NA, color = "black", linetype = "dashed")

dpl_s2 <- dpl_s2 +
  geom_rect(aes(xmin = 0.7, ymin = -0.25, xmax = 7.3, ymax = 2.2), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 7.7, ymin = -0.25, xmax = 8.3, ymax = 2.2), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 8.7, ymin = -0.25, xmax = 9.3, ymax = 2.2), 
            fill = NA, color = "black", linetype = "dashed")

dpl_s3 <- dpl_s3 +
  geom_rect(aes(xmin = 0.7, ymin = -0.25, xmax = 7.3, ymax = 2.5), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 7.7, ymin = -0.25, xmax = 8.3, ymax = 2.5), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 8.7, ymin = -0.25, xmax = 9.3, ymax = 2.5), 
            fill = NA, color = "black", linetype = "dashed")

dpl_s4 <- dpl_s4 +
  geom_rect(aes(xmin = 0.7, ymin = -0.25, xmax = 1.3, ymax = 2.83), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 1.7, ymin = -0.25, xmax = 2.3, ymax = 2.83), 
            fill = NA, color = "black", linetype = "dashed") +
  geom_rect(aes(xmin = 2.7, ymin = -0.25, xmax = 9.3, ymax = 2.83), 
            fill = NA, color = "black", linetype = "dashed")

grid.arrange(dpl_s1,dpl_s2,dpl_s3,dpl_s4,nrow=2)
```

For each dendrogram, we split the models into three groups. Most high count and low count dendrograms include two single-model groups that consist of a model without day of the week effects.  However, this does not seem to be indicative of any trends in model similarity related to day of the week effect, more just that Google_Harvard-CPF and Covid19Sim-Simulator tend to be more different from the other models over all. If we were to create the dendrograms without Google_Harvard-CPF, then JHUAPL-Bucky, a model that includes day of the week effects, would become its own group. The low count 3-week horizon dendrogram split into four groups more easily than three groups; however, the scale for differences in Cramér Distance for the low count locations is very small, likely a result of low incident hospitalizations, which may this difference. 


For Saturday forecasts, Google_Harvard-CPF is consistently the most dissimilar from other models, followed by Covid19Sim-Simulator, across almost all horizons for both high-count and low count regions. This is the same as for Thursday forecasts. For Saturday forecasts, there is also not an obvious conclusion to be drawn about the impact of day of week effects on model similarity. 


## Results and Conclusion

Although this investigation of model similarity and day of the week effects for Covid-19 forecasts yielded largely inconclusive results, there are several clear takeaways. First, Cramér's Distance is an attractive metric for measuring model similarity, especially for the models that submit to the COVID-19 Forecast Hub, because it is compatible with WIS and approximations are relatively easy to calculate. Second, the issue of unaligned hospitalization forecasts can be solved fairly easily by creating a new relative horizon variable. In addition, the potential disadvantage of certain models having an unfair forecasting advantage if we use this variable is unfounded for the nine models analyzed (and likely several other incident hospitalization models). Third, incident hospitalization truth data shows day of the week effects in which weekends have noticeably lower counts. Some of the models account for this day of week effect while others do not. Finally, day of week effects may impact model similarity to an extent but results are generally inconclusive, and further research is needed to draw any stronger and/or more definitive conclusions.

There are many directions that future research could take this work, but two stand out at the time of writing this report. One would be to investigate the relationship between the performance of the analyzed models and their similarity by comparing relative WIS. The other would be to explore the decomposition of Cramér's Distance and discover how much shape, spread, and mean of the distributions of forecasts play into the approximate pairwise distance.   

Lastly, the repository for this project is available on [Github](https://github.com/NutchaW/covid19_forecast_similarity).
